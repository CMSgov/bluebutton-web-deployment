---
- name: Using IBM Storage Virtualize collection to migrate Global Mirror, GMCV relationships with a prefix to PBR
  hosts: localhost
  gather_facts: false
  vars_files:
    - inventory.ini
  vars:
    location_1_iogrp: "{{ location_1_iogrp_id | default(0) }}"
    location_2_iogrp: "{{ location_2_iogrp_id | default(0) }}"
    existing_certificate: "{{ use_existing_certificate | default(true) }}"
    cleanup: "{{ remove_aux_volumes | default(false) }}"
    logpath: "{{ log_path | default('./gmcv_pbr_migration.log') }}"
  connection: local
  tasks:
    # ------------
    # Fetch tokens from both for authentication
    - name: Fetch auth token for master
      register: master_auth
      ibm.storage_virtualize.ibm_svc_auth:
        clustername: "{{ master_clustername }}"
        username: "{{ master_username }}"
        password: "{{ master_password }}"
        log_path: "{{ logpath }}"
    - name: Fetch auth token for aux
      register: aux_auth
      ibm.storage_virtualize.ibm_svc_auth:
        clustername: "{{ aux_clustername }}"
        username: "{{ aux_username }}"
        password: "{{ aux_password }}"
        log_path: "{{ logpath }}"
    # ------------
    # Get system details for both systems
    - name: Get system name for master
      register: master_system
      ibm.storage_virtualize.ibm_svc_info:
        clustername: "{{ master_clustername }}"
        token: "{{ master_auth.token }}"
        gather_subset: system
        log_path: "{{ logpath }}"
    - name: Get system name for aux
      register: aux_system
      ibm.storage_virtualize.ibm_svc_info:
        clustername: "{{ aux_clustername }}"
        token: "{{ aux_auth.token }}"
        gather_subset: system
        log_path: "{{ logpath }}"
    # ------------
    # Verify partnership exists between systems and fetch details
    - name: Fetch Partnership details
      register: partnership_info
      ibm.storage_virtualize.ibm_svc_info:
        clustername: "{{ master_clustername }}"
        token: "{{ master_auth.token }}"
        gather_subset: partnership
        objectname: "{{ aux_system.System.name }}"
        log_path: "{{ logpath }}"
    - name: Fail if partnership is not found between master and aux
      when: partnership_info is not defined or partnership_info.Partnership == None
      ansible.builtin.fail:
        msg: "Partnership not found between master and aux"
    # ------------
    # Verify required pools exist on both systems and fetch details
    - name: Get master_pool details
      register: master_pool_info
      ibm.storage_virtualize.ibm_svc_info:
        clustername: "{{ master_clustername }}"
        token: "{{ master_auth.token }}"
        gather_subset: pool
        objectname: "{{ master_pool_name }}"
        log_path: "{{ logpath }}"
    - name: Get aux_pool details
      register: aux_pool_info
      ibm.storage_virtualize.ibm_svc_info:
        clustername: "{{ aux_clustername }}"
        token: "{{ aux_auth.token }}"
        gather_subset: pool
        objectname: "{{ aux_pool_name }}"
        log_path: "{{ logpath }}"
    # ------------
    # All prerequisites satisfied
    - name: Link specified pools
      ibm.storage_virtualize.ibm_svc_mdiskgrp:
        clustername: "{{ master_clustername }}"
        token: "{{ master_auth.token }}"
        name: "{{ master_pool_name }}"
        state: present
        replicationpoollinkuid: "{{ aux_pool_info.Pool.replication_pool_link_uid }}"
        replication_partner_clusterid: "{{ partnership_info.Partnership.id }}"
        log_path: "{{ logpath }}"
    # ------------
    # Create certificates if specified
    - name: Create self signed certificates
      when: not existing_certificate
      block:
        - name: Generate certificate on master
          ibm.storage_virtualize.ibm_svctask_command:
            clustername: "{{ master_clustername }}"
            username: "{{ master_username }}"
            password: "{{ master_password }}"
            command: "svctask chsystemcert -mksystemsigned"
            log_path: "{{ logpath }}"
        - name: Generate certificate on aux
          ibm.storage_virtualize.ibm_svctask_command:
            clustername: "{{ aux_clustername }}"
            username: "{{ aux_username }}"
            password: "{{ aux_password }}"
            command: "svctask chsystemcert -mksystemsigned"
            log_path: "{{ logpath }}"
    # ------------
    # Export these certificates locally
    - name: Export system certificate on master
      ibm.storage_virtualize.ibm_sv_manage_ssl_certificate:
        clustername: "{{ master_clustername }}"
        token: "{{ master_auth.token }}"
        certificate_type: "system"
        log_path: "{{ logpath }}"
    - name: Export system certificate on aux
      ibm.storage_virtualize.ibm_sv_manage_ssl_certificate:
        clustername: "{{ aux_clustername }}"
        token: "{{ aux_auth.token }}"
        certificate_type: "system"
        log_path: "{{ logpath }}"
    # ------------
    # Create and manage truststores
    - name: Create Truststore on master and get aux certificate on master
      ibm.storage_virtualize.ibm_sv_manage_truststore_for_replication:
        clustername: "{{ master_clustername }}"
        username: "{{ master_username }}"
        password: "{{ master_password }}"
        remote_clustername: "{{ aux_clustername }}"
        remote_username: "{{ aux_username }}"
        remote_password: "{{ aux_password }}"
        name: "{{ truststore_name }}"
        state: present
        restapi: "on"
        log_path: "{{ logpath }}"
    - name: Create Truststore on aux and get master certificate on aux
      ibm.storage_virtualize.ibm_sv_manage_truststore_for_replication:
        clustername: "{{ aux_clustername }}"
        username: "{{ aux_username }}"
        password: "{{ aux_password }}"
        remote_clustername: "{{ master_clustername }}"
        remote_username: "{{ master_username }}"
        remote_password: "{{ master_password }}"
        name: "{{ truststore_name }}"
        state: present
        restapi: "on"
        log_path: "{{ logpath }}"
    # ------------
    - name: Create replication policy between 2 systems
      ibm.storage_virtualize.ibm_sv_manage_replication_policy:
        clustername: "{{ master_clustername }}"
        token: "{{ master_auth.token }}"
        name: "{{ replication_policy_name }}"
        topology: 2-site-async-dr
        location1system: "{{ master_system.System.name }}"
        location1iogrp: "{{ location_1_iogrp }}"
        location2system: "{{ aux_system.System.name }}"
        location2iogrp: "{{ location_2_iogrp }}"
        rpoalert: "{{ rpo_alert }}"
        state: present
        log_path: "{{ logpath }}"
    # ------------
    # Enable PBR on both sides of partnership
    - name: Enable PBR on both sides of partnership
      when: partnership_info.Partnership.pbr_in_use == "no"
      block:
        - name: Enable PBR on partnership on master
          ibm.storage_virtualize.ibm_sv_manage_fc_partnership:
            clustername: "{{ master_clustername }}"
            token: "{{ master_auth.token }}"
            remote_clustername: "{{ aux_clustername }}"
            remote_token: "{{ aux_auth.token }}"
            remote_system: "{{ aux_system.System.name }}"
            linkbandwidthmbits: "{{ partnership_info.Partnership.link_bandwidth_mbits }}"
            backgroundcopyrate: "{{ partnership_info.Partnership.background_copy_rate }}"
            state: present
            start: true
            pbrinuse: "yes"
            log_path: "{{ logpath }}"
        - name: Enable PBR on partnership on aux
          ibm.storage_virtualize.ibm_sv_manage_fc_partnership:
            clustername: "{{ aux_clustername }}"
            token: "{{ aux_auth.token }}"
            remote_clustername: "{{ master_clustername }}"
            remote_token: "{{ master_auth.token }}"
            remote_system: "{{ master_system.System.name }}"
            linkbandwidthmbits: "{{ partnership_info.Partnership.link_bandwidth_mbits }}"
            backgroundcopyrate: "{{ partnership_info.Partnership.background_copy_rate }}"
            state: present
            start: true
            pbrinuse: "yes"
            log_path: "{{ logpath }}"
    # ------------
    # Get relationship details
    - name: Get relationships with prefix information
      register: rels_info
      ibm.storage_virtualize.ibm_svc_info:
        clustername: "{{ master_clustername }}"
        token: "{{ master_auth.token }}"
        gather_subset: rcrelationship
        filtervalue: "name={{ rel_name_prefix }}*:aux_cluster_id={{ aux_system.System.id }}:copy_type=global"
        log_path: "{{ logpath }}"
    # For delayed cleanup
    - name: Get common variables in inventory_cleanup.ini file
      ansible.builtin.copy:
        content: |
          master_clustername: "{{ master_clustername }}"
          master_system_name: "{{ master_system.System.name }}"
          master_username: "ADD MASTER USERNAME HERE"
          master_password: "ADD MASTER PASSWORD HERE"
          aux_clustername: "{{ aux_clustername }}"
          aux_system_name: "{{ aux_system.System.name }}"
          aux_username: "ADD AUX USERNAME HERE"
          aux_password: "ADD AUX PASSWORD HERE"
          relationships_to_cleanup:

        dest: ./inventory_cleanup_prefix_{{ master_system.System.name }}_{{ aux_system.System.name }}.ini
        mode: "0644"
      when: not cleanup
    # ------------
    # Loop over found relationships and migrate each one individually
    - name: Loop over relationships and migrate each one
      ansible.builtin.include_tasks: _gm_gmcv_prefix_pbr_migrate.yaml
      loop: "{{ rels_info.RemoteCopy }}"
    # ------------
    # Display message to add credentials to inventory file, and add inventory file name to cleanup playbook
    - name: Display message to add credentials to inventory file, and add inventory file name to cleanup playbook
      ansible.builtin.debug:
        msg: "A file named inventory_cleanup_prefix_{{ master_system.System.name }}_{{ aux_system.System.name }}.ini has been created.
              Due to security concerns, system credentials have not been added to the file.
              Before running the cleanup playbook, add these credentials to the file,
              and add the name of the file to vars_files section of the cleanup playbook"
      when: not cleanup
