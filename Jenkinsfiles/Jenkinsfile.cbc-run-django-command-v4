pipeline {
  agent {
    kubernetes {
      defaultContainer "bb2-cbc-build"
      yamlFile "Jenkinsfiles/cbc-build.yaml"
    }
  }

  environment {
    AWS_DEFAULT_REGION = "us-east-1"
    SSH_KEY = credentials("bb2-ssh-key-${params.APP_ENV}-V4")
  }

  parameters {
    choice(
      name: "APP_ENV",
      choices: ["test", "impl", "prod"],
      description: "The environment to run in."
    )

    string(
      name: "DJANGO_COMMAND",
      defaultValue: "",
      description: "The django management command to run."
    )

    string(
      name: "CANARY_IP",
      defaultValue: "",
      description: "When this is set, run against a canary IP rather than a tagged deployment instance"
    )
  }

  stages {
    stage("Notify Slack") {
      steps {
        script {
          helpers = load "Jenkinsfiles/helpers.groovy"
          helpers.slackNotify "STARTING - ENV:${params.APP_ENV}"
        }
      }
    }

    stage("Assume AWS Role") {
      steps {
        withCredentials([string(credentialsId: params.APP_ENV == 'test' ? 'aws-assume-role-arn' : 'aws-assume-role-arn-prod', variable: 'ROLE_ARN')]) {
          script {
            def sessionName = "jenkins-${env.BUILD_ID}"
            def credsJson = sh(
              script: """#!/bin/bash
              aws sts assume-role \\
                --role-arn "$ROLE_ARN" \\
                --role-session-name "$sessionName" \\
                --output json
              """,
              returnStdout: true
            ).trim()

            def creds = readJSON text: credsJson

            env.AWS_ACCESS_KEY_ID     = creds.Credentials.AccessKeyId
            env.AWS_SECRET_ACCESS_KEY = creds.Credentials.SecretAccessKey
            env.AWS_SESSION_TOKEN     = creds.Credentials.SessionToken
            // --- ADD THESE DEBUG LINES ---
            echo "Assumed Role ARN: ${ROLE_ARN}"
            echo "Assumed Access Key ID (partial): ${env.AWS_ACCESS_KEY_ID.substring(0, 8)}..." // Print first 8 chars
            echo "Assumed Session Token (partial): ${env.AWS_SESSION_TOKEN.substring(0, 8)}..." // Print first 8 chars
            echo "Assumed Role Expiration: ${creds.Credentials.Expiration}" // Print expiration time
            // DO NOT print env.AWS_SECRET_ACCESS_KEY directly as it's highly sensitive
            def callerIdentity = sh(
              script: """#!/bin/bash
              aws sts get-caller-identity --output text
              ansible-inventory -i inventory/aws_ec2.yaml --graph
              """,
              returnStatus: true
            )

            if (callerIdentity != 0) {
              error("Failed to verify assumed AWS role. Check credentials or permissions.")
            } else {
              echo "Successfully verified assumed AWS role: ${sh(script: 'aws sts get-caller-identity', returnStdout: true).trim()}"
            }
          }
        }
      }
    }

    stage("Determine Ansible Config") {
      steps {
        script {
          if (params.CANARY_IP != "") {
            echo("Setting ansible config for canary IP")
            ANSIBLE_INVENTORY = "${params.CANARY_IP},"
            ANSIBLE_SUBSET = ""
          } else {
            echo("Setting ansible config for tagged deployment instance")
            ANSIBLE_INVENTORY = "inventory/aws_ec2.yaml"
            ANSIBLE_SUBSET = "tag_Function_app_AppServer"
          }
          // For debugging, print what's being set:
            echo "ANSIBLE_INVENTORY: ${ANSIBLE_INVENTORY}"
            echo "ANSIBLE_SUBSET: ${ANSIBLE_SUBSET}"
        }
      }
    }

    stage("Run Django Command") {
      steps {
       withEnv([
      'ANSIBLE_TIMEOUT=30',
    ]) {
      sh """
      set -ex

      echo "--- PRE-CLEANUP: ANSIBLE VERSION & PATHS ---"
      ansible --version

      echo "--- PRE-CLEANUP: LISTING amazon.aws (ansible-galaxy) ---"
      ansible-galaxy collection list amazon.aws || echo "INFO: amazon.aws not found by ansible-galaxy pre-cleanup."

      # --- AGGRESSIVE REMOVAL OF EXISTING amazon.aws COLLECTIONS ---
      echo "Attempting to remove existing amazon.aws collections..."
      # Try removing from various common paths. || true allows the script to continue if a command fails (e.g., path doesn't exist)
      (ansible-galaxy collection remove amazon.aws --paths /root/.ansible/collections || echo "Failed/Skipped: Remove from /root/.ansible/collections")
      (ansible-galaxy collection remove amazon.aws --paths /usr/local/lib/python3.11/site-packages/ansible_collections || echo "Failed/Skipped: Remove from site-packages collection dir")
      (ansible-galaxy collection remove amazon.aws --paths /usr/share/ansible/collections || echo "Failed/Skipped: Remove from /usr/share/ansible/collections")

      # Direct rm -rf for good measure (use with caution, ensure paths are correct)
      (rm -rf /root/.ansible/collections/ansible_collections/amazon/aws || echo "No /root/.ansible/collections/ansible_collections/amazon/aws to remove.")
      (rm -rf /usr/local/lib/python3.11/site-packages/ansible_collections/amazon/aws || echo "No /usr/local/lib/python3.11/site-packages/ansible_collections/amazon/aws to remove.")
      (rm -rf /usr/share/ansible/collections/ansible_collections/amazon/aws || echo "No /usr/share/ansible/collections/ansible_collections/amazon/aws to remove.")
      echo "Removal attempts finished."

      echo "--- POST-CLEANUP: LISTING amazon.aws (ansible-galaxy) ---"
      # This should ideally show no amazon.aws installed now, or an error if the command itself fails.
      ansible-galaxy collection list amazon.aws || echo "INFO: amazon.aws not found by ansible-galaxy post-cleanup (expected)."

      # --- INSTALL A SUPPORTED VERSION OF amazon.aws ---
      # Let's try a known stable older version first, e.g., 7.3.0
      # Or you can try 9.0.0 or 9.1.0 again if you suspect the previous install attempt (in Docker) was just flawed.
      AWS_COLLECTION_TO_INSTALL="amazon.aws:7.3.0" # Or try "amazon.aws:9.0.0" or "amazon.aws:9.1.0"
      echo "Installing \$AWS_COLLECTION_TO_INSTALL ..."
      # --force will overwrite if anything unexpected remains.
      # This will typically install to /root/.ansible/collections as we are running as root.
      ansible-galaxy collection install \$AWS_COLLECTION_TO_INSTALL --force -vvv # Added -vvv for galaxy install verbosity

      echo "--- POST-INSTALL: ANSIBLE VERSION & PATHS ---"
      # If galaxy installed to /root/.ansible/collections, this should reflect that as a primary path.
      ansible --version

      echo "--- POST-INSTALL: LISTING amazon.aws (ansible-galaxy) ---"
      ansible-galaxy collection list amazon.aws || echo "ERROR: \$AWS_COLLECTION_TO_INSTALL still not found post-install!"

      echo "--- POST-INSTALL: CHECKING PHYSICAL FILE FOR aws_ssm connection plugin ---"
      # Check the typical user install path for root. Adjust if galaxy installs elsewhere by default in your image.
      # This assumes ansible-galaxy installs to /root/.ansible/collections for the root user.
      COLLECTION_INSTALL_PATH="\$(ansible --version | grep 'ansible collection location' | head -n1 | awk -F'= ' '{print \$2}' | cut -d: -f1)"
      # If COLLECTION_INSTALL_PATH is empty or wrong, default to a common path
      if [ -z "\$COLLECTION_INSTALL_PATH" ]; then
          COLLECTION_INSTALL_PATH="/root/.ansible/collections"
      fi
      echo "Expected collection install path: \$COLLECTION_INSTALL_PATH"
      ls -l "\$COLLECTION_INSTALL_PATH/ansible_collections/amazon/aws/plugins/connection/aws_ssm.py" || echo "ERROR: aws_ssm.py file not found post-install!"

      echo "--- POST-INSTALL: TRYING TO GET PLUGIN DOC (ansible-doc for connection plugin) ---"
      ansible-doc -t connection amazon.aws.aws_ssm || echo "ERROR: aws_ssm connection plugin NOT found by ansible-doc post-install!"

      echo "--- POST-INSTALL: TRYING TO GET PLUGIN DOC (ansible-doc for inventory plugin - as a control test) ---"
      ansible-doc -t inventory amazon.aws.aws_ec2 || echo "ERROR: aws_ec2 inventory plugin NOT found by ansible-doc post-install!"

      echo "--- RUNNING PLAYBOOK ---"
      ansible-playbook -vvv playbook/run_django_command/main.yml \\
        --connection amazon.aws.aws_ssm \\
        -i '${ANSIBLE_INVENTORY}' \\
        -l '${ANSIBLE_SUBSET}' \\
        -e 'env=${APP_ENV}' \\
        -e 'django_command="${DJANGO_COMMAND}"'
      """
        }
      }
    }
  }

  post {
    success {
      script {
        helpers.slackNotify("SUCCESS - ENV:${params.APP_ENV}", "good")
      }
    }

    failure {
      script {
        helpers.slackNotify("FAILURE - ENV:${params.APP_ENV}", "bad")
      }
    }
  }
}